import os
import torch
from flask import Flask, request, jsonify
from flask_cors import CORS
from transformers import AutoTokenizer, AutoModelForCausalLM
import logging

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
MODEL_NAME = "baseet/Holomai-Dream-Model"
HF_TOKEN = "hf_PurgeraWpxR0gUCYFgccKDnFqTFneYSCSw"

# Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
tokenizer = None
model = None

def load_model():
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„ØªÙˆÙƒÙŠÙ†Ø§ÙŠØ²Ø±"""
    global tokenizer, model
    
    try:
        logger.info(f"ğŸ”„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {MODEL_NAME}")
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙˆÙƒÙŠÙ†Ø§ÙŠØ²Ø±
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_NAME, 
            token=HF_TOKEN,
            trust_remote_code=True
        )
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ pad_token Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            token=HF_TOKEN,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None,
            trust_remote_code=True
        )
        
        if device == "cpu":
            model = model.to(device)
        
        logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­ Ø¹Ù„Ù‰ {device}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {str(e)}")
        return False

def test_model(text):
    """Ø¯Ø§Ù„Ø© ØªÙØ³ÙŠØ± Ø§Ù„Ø£Ø­Ù„Ø§Ù…"""
    try:
        if model is None or tokenizer is None:
            return "âŒ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…Ø­Ù…Ù„"
        
        # ØªØ±Ù…ÙŠØ² Ø§Ù„Ù†Øµ
        inputs = tokenizer(
            text, 
            return_tensors="pt", 
            truncation=True, 
            max_length=512,
            padding=True
        )
        
        # Ù†Ù‚Ù„ Ø¥Ù„Ù‰ Ù†ÙØ³ Ø¬Ù‡Ø§Ø² Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        device = next(model.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=200,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.1
            )
        
        # ÙÙƒ ØªØ±Ù…ÙŠØ² Ø§Ù„Ù†ØªÙŠØ¬Ø©
        result = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ
        if text in result:
            result = result.replace(text, "").strip()
        
        return result if result else "ØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙØ³ÙŠØ± Ø¨Ù†Ø¬Ø§Ø­"
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙØ³ÙŠØ±: {str(e)}")
        return f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {str(e)}"

@app.route('/')
def home():
    """Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    return jsonify({
        "status": "ok",
        "message": "ğŸŒ™ Ø®Ø§Ø¯Ù… ØªÙØ³ÙŠØ± Ø§Ù„Ø£Ø­Ù„Ø§Ù… ÙŠØ¹Ù…Ù„ Ø¹Ù„Ù‰ Railway",
        "model": MODEL_NAME,
        "model_loaded": model is not None,
        "device": "cuda" if torch.cuda.is_available() else "cpu"
    })

@app.route('/health')
def health():
    """ÙØ­Øµ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø§Ø¯Ù…"""
    return jsonify({
        "status": "healthy",
        "model_loaded": model is not None,
        "gpu_available": torch.cuda.is_available()
    })

@app.route('/predict', methods=['POST'])
def predict():
    """ØªÙØ³ÙŠØ± Ø§Ù„Ø­Ù„Ù…"""
    try:
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        if model is None or tokenizer is None:
            return jsonify({
                "error": "Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…Ø­Ù…Ù„ Ø­Ø§Ù„ÙŠØ§Ù‹. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨Ø¹Ø¯ Ù‚Ù„ÙŠÙ„."
            }), 503
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        data = request.get_json()
        
        if not data or 'text' not in data:
            return jsonify({
                "error": "ÙŠØ±Ø¬Ù‰ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù†Øµ ÙÙŠ Ø­Ù‚Ù„ 'text'"
            }), 400
        
        dream_text = data['text'].strip()
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø·ÙˆÙ„ Ø§Ù„Ù†Øµ
        if len(dream_text) < 5:
            return jsonify({
                "error": "ÙŠØ±Ø¬Ù‰ ÙƒØªØ§Ø¨Ø© ÙˆØµÙ Ø£ÙƒØ«Ø± ØªÙØµÙŠÙ„Ø§Ù‹ Ù„Ù„Ø­Ù„Ù…"
            }), 400
        
        logger.info(f"ğŸ”® Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨ ØªÙØ³ÙŠØ±: {dream_text[:50]}...")
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙØ³ÙŠØ±
        interpretation = test_model(dream_text)
        
        return jsonify({
            "result": interpretation,
            "status": "success"
        })
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {str(e)}")
        return jsonify({
            "error": f"Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {str(e)}"
        }), 500

if __name__ == '__main__':
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„
    logger.info("ğŸš€ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø®Ø§Ø¯Ù… ØªÙØ³ÙŠØ± Ø§Ù„Ø£Ø­Ù„Ø§Ù… Ø¹Ù„Ù‰ Railway...")
    
    # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model_loaded = load_model()
    if not model_loaded:
        logger.warning("âš ï¸ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù†Ø¯ Ø§Ù„Ø¨Ø¯Ø¡")
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙˆØ±Øª Ù…Ù† Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø©
    port = int(os.environ.get('PORT', 8000))
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø®Ø§Ø¯Ù…
    app.run(host='0.0.0.0', port=port, debug=False)